"""
æ¨¡å‹é€‚é…å™¨ï¼šæ”¯æŒæœ¬åœ° Ollamaï¼ˆHTTP ä¼˜å…ˆï¼ŒCLI å›é€€ï¼‰ã€‚
æ­¤ç‰ˆæœ¬å·²ç§»é™¤ mock å®ç°ï¼Œè°ƒç”¨å¤±è´¥å°†æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿æµ‹è¯•æœ¬åœ° Ollama è¡Œä¸ºã€‚
"""
import json
import time
from typing import Dict
import subprocess
import requests
from requests.exceptions import RequestException
import traceback
from src import config_manager as config
from src.utils.logger import logger


def mock_generate_plan(agent_id: str, issue_text: str) -> Dict:
    # è¿”å›ç¬¦åˆ PlanSchema çš„å­—å…¸ç¤ºä¾‹
    return {
        "id": f"{agent_id}-plan1",
        "core_idea": f"åŸºäº{issue_text}çš„åˆæ­¥æ€è·¯",
        "steps": ["æ­¥éª¤1ï¼šè°ƒç ”", "æ­¥éª¤2ï¼šè¯•ç‚¹", "æ­¥éª¤3ï¼šæ¨å¹¿"],
        "feasibility": {"advantages": ["ä½æˆæœ¬"], "requirements": ["èµ„æºA", "èµ„æºB"]},
        "limitations": ["æ—¶é—´è¾ƒé•¿"]
    }





def call_ollama_http(model: str, prompt: str, timeout: int = 60, stream: bool = False):
    """å°è¯•é€šè¿‡ Ollama çš„æœ¬åœ° HTTP API è°ƒç”¨æ¨¡å‹ã€‚"""
    url = config.OLLAMA_HTTP_URL
    payload = {"model": model, "prompt": prompt, "stream": stream}
    try:
        logger.info(f"Ollama HTTP call starting to {url} (stream={stream})...")
        resp = requests.post(url, json=payload, timeout=timeout, stream=stream)
        logger.info(f"Ollama HTTP response received (status: {resp.status_code})")
        resp.raise_for_status()
        if stream:
            return resp
        data = resp.json()
        return data.get("response", "")
    except RequestException as e:
        raise RuntimeError(f"Ollama HTTP request failed: {e}")


def call_ollama_cli(model: str, prompt: str, timeout: int = 30) -> str:
    """å°è¯•ä½¿ç”¨æœ¬åœ° ollama CLIï¼ˆè‹¥å·²å®‰è£…ï¼‰ä½œä¸ºå›é€€æ–¹æ¡ˆã€‚"""
    try:
        # ollama run <model> "<prompt>"
        completed = subprocess.run(["ollama", "run", model, prompt], capture_output=True, text=True, timeout=timeout)
        if completed.returncode != 0:
            raise RuntimeError(f"ollama CLI error: {completed.stderr.strip()}")
        return completed.stdout
    except FileNotFoundError:
        raise RuntimeError("ollama CLI not found on PATH")


def call_model_with_ollama_fallback(model: str, prompt: str) -> str:
    """ä¼˜å…ˆå°è¯• HTTPï¼Œç„¶åå›é€€åˆ° CLIã€‚å¦‚æœéƒ½å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸ã€‚"""
    # é¦–é€‰ HTTP API
    try:
        return call_ollama_http(model, prompt)
    except Exception:
        # å›é€€åˆ° CLI
        return call_ollama_cli(model, prompt)


def call_deepseek(model: str, prompt: str, timeout: int = 60, max_retries: int = 3, stream: bool = False, tools: list = None, tool_choice: str = "auto"):
    """è°ƒç”¨ DeepSeek API (OpenAI å…¼å®¹æ¥å£)ï¼Œå¸¦é‡è¯•æœºåˆ¶ã€‚
    
    Args:
        model: æ¨¡å‹åç§°
        prompt: æç¤ºè¯æˆ–æ¶ˆæ¯åˆ—è¡¨
        timeout: è¶…æ—¶æ—¶é—´
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        stream: æ˜¯å¦æµå¼è¾“å‡º
        tools: OpenAI Function Callingæ ¼å¼çš„å·¥å…·å®šä¹‰åˆ—è¡¨
        tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥ ("auto"/"none"æˆ–ç‰¹å®šå·¥å…·å)
    """
    url = f"{config.DEEPSEEK_BASE_URL}/chat/completions"
    headers = {
        "Authorization": f"Bearer {config.DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # æ¸…ç†å¯èƒ½å¯¼è‡´ JSON è§£æå¤±è´¥çš„ä¸å¯è§æ§åˆ¶å­—ç¬¦ï¼Œå¹¶è®°å½•é•¿åº¦
    if isinstance(prompt, str):
        clean_prompt = "".join(c for c in prompt if c.isprintable() or c in "\n\r\t")
        logger.info(f"DeepSeek prompt length: {len(clean_prompt)} characters")
        messages = [{"role": "user", "content": clean_prompt}]
    else:
        # æ”¯æŒç›´æ¥ä¼ å…¥æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
        messages = prompt
        logger.info(f"DeepSeek messages count: {len(messages)}")
    
    payload = {
        "model": model or config.DEEPSEEK_MODEL,
        "messages": messages,
        "stream": stream
    }
    
    # æ·»åŠ å·¥å…·è°ƒç”¨æ”¯æŒ
    if tools:
        payload["tools"] = tools
        payload["tool_choice"] = tool_choice
        logger.info(f"DeepSeek: Enabled function calling with {len(tools)} tools")
    
    last_exception = None
    for attempt in range(max_retries):
        try:
            logger.info(f"DeepSeek API call starting (attempt {attempt + 1}/{max_retries}, stream={stream})...")
            resp = requests.post(url, json=payload, headers=headers, timeout=timeout, stream=stream)
            logger.info(f"DeepSeek API response received (status: {resp.status_code})")
            if resp.status_code != 200:
                try:
                    error_detail = resp.json()
                    logger.error(f"DeepSeek API error detail: {error_detail}")
                except:
                    logger.error(f"DeepSeek API error body: {resp.text}")
            resp.raise_for_status()
            if stream:
                return resp
            data = resp.json()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰tool_calls
            message = data["choices"][0]["message"]
            if message.get("tool_calls"):
                logger.info(f"DeepSeek returned {len(message['tool_calls'])} tool calls")
                return message  # è¿”å›å®Œæ•´æ¶ˆæ¯ï¼ˆåŒ…å«tool_callsï¼‰
            
            return message.get("content", "")
        except RequestException as e:
            last_exception = e
            logger.warning(f"DeepSeek API attempt {attempt + 1} failed: {e}. Retrying...")
            if attempt < max_retries - 1:
                time.sleep(2 * (attempt + 1))  # ç®€å•çš„æŒ‡æ•°é€€é¿
            
    raise RuntimeError(f"DeepSeek API request failed after {max_retries} attempts: {last_exception}")


def call_aliyun(model: str, prompt: str, timeout: int = 60, max_retries: int = 3, stream: bool = False):
    """è°ƒç”¨é˜¿é‡Œäº‘ DashScope API (OpenAI å…¼å®¹æ¥å£)ï¼Œå¸¦é‡è¯•æœºåˆ¶ã€‚"""
    url = f"{config.ALIYUN_BASE_URL}/chat/completions"
    headers = {
        "Authorization": f"Bearer {config.ALIYUN_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": model or config.ALIYUN_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "stream": stream
    }
    
    last_exception = None
    for attempt in range(max_retries):
        try:
            logger.info(f"Aliyun API call starting (attempt {attempt + 1}/{max_retries}, stream={stream})...")
            resp = requests.post(url, json=payload, headers=headers, timeout=timeout, stream=stream)
            logger.info(f"Aliyun API response received (status: {resp.status_code})")
            resp.raise_for_status()
            if stream:
                return resp
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except RequestException as e:
            last_exception = e
            logger.warning(f"Aliyun API attempt {attempt + 1} failed: {e}. Retrying...")
            if attempt < max_retries - 1:
                time.sleep(2 * (attempt + 1))
            
    raise RuntimeError(f"Aliyun API request failed after {max_retries} attempts: {last_exception}")


def call_openai(model: str, prompt: str, timeout: int = 60, max_retries: int = 3, stream: bool = False, tools: list = None, tool_choice: str = "auto"):
    """è°ƒç”¨ OpenAI APIï¼Œå¸¦é‡è¯•æœºåˆ¶ã€‚
    
    Args:
        model: æ¨¡å‹åç§°
        prompt: æç¤ºè¯æˆ–æ¶ˆæ¯åˆ—è¡¨
        timeout: è¶…æ—¶æ—¶é—´
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        stream: æ˜¯å¦æµå¼è¾“å‡º
        tools: OpenAI Function Callingæ ¼å¼çš„å·¥å…·å®šä¹‰åˆ—è¡¨
        tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥
    """
    url = f"{config.OPENAI_BASE_URL}/chat/completions"
    headers = {
        "Authorization": f"Bearer {config.OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # æ”¯æŒå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨
    if isinstance(prompt, str):
        messages = [{"role": "user", "content": prompt}]
    else:
        messages = prompt
        
    payload = {
        "model": model or config.OPENAI_MODEL,
        "messages": messages,
        "stream": stream
    }
    
    # æ·»åŠ å·¥å…·è°ƒç”¨æ”¯æŒ
    if tools:
        payload["tools"] = tools
        payload["tool_choice"] = tool_choice
        logger.info(f"OpenAI: Enabled function calling with {len(tools)} tools")
    
    
    last_exception = None
    for attempt in range(max_retries):
        try:
            logger.info(f"OpenAI API call starting (attempt {attempt + 1}/{max_retries}, stream={stream})...")
            resp = requests.post(url, json=payload, headers=headers, timeout=timeout, stream=stream)
            logger.info(f"OpenAI API response received (status: {resp.status_code})")
            resp.raise_for_status()
            if stream:
                return resp
            data = resp.json()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰tool_calls
            message = data["choices"][0]["message"]
            if message.get("tool_calls"):
                logger.info(f"OpenAI returned {len(message['tool_calls'])} tool calls")
                return message  # è¿”å›å®Œæ•´æ¶ˆæ¯ï¼ˆåŒ…å«tool_callsï¼‰
            
            return message.get("content", "")
        except RequestException as e:
            last_exception = e
            logger.warning(f"OpenAI API attempt {attempt + 1} failed: {e}. Retrying...")
            if attempt < max_retries - 1:
                time.sleep(2 * (attempt + 1))
            
    raise RuntimeError(f"OpenAI API request failed after {max_retries} attempts: {last_exception}")


def call_openrouter(model: str, prompt: str, timeout: int = 120, max_retries: int = 3, stream: bool = False, reasoning: dict = None):
    """è°ƒç”¨ OpenRouter APIï¼Œå¸¦é‡è¯•æœºåˆ¶ã€‚"""
    # å¦‚æœæä¾›äº† reasoningï¼Œä½¿ç”¨ /responses æ¥å£ (OpenRouter ç»Ÿä¸€å“åº”æ¥å£)
    if reasoning:
        url = f"{config.OPENROUTER_BASE_URL.replace('/chat/completions', '')}/responses"
        payload = {
            "model": model or config.OPENROUTER_MODEL,
            "input": prompt,
            "reasoning": reasoning,
            "stream": stream
        }
    else:
        url = f"{config.OPENROUTER_BASE_URL}/chat/completions"
        payload = {
            "model": model or config.OPENROUTER_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "stream": stream
        }
    
    headers = {
        "Authorization": f"Bearer {config.OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://github.com/AICouncil", # é€‰å¡«
        "X-Title": "AICouncil" # é€‰å¡«
    }
    
    last_exception = None
    for attempt in range(max_retries):
        try:
            logger.info(f"OpenRouter API call starting (attempt {attempt + 1}/{max_retries}, stream={stream}, reasoning={reasoning})...")
            logger.info(f"OpenRouter prompt length: {len(prompt)} characters")
            
            # å¯¹äºæ¨ç†æ¨¡å‹ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
            current_timeout = timeout
            if reasoning:
                current_timeout = max(timeout, 180) # è‡³å°‘ 3 åˆ†é’Ÿ
                
            resp = requests.post(url, json=payload, headers=headers, timeout=current_timeout, stream=stream)
            logger.info(f"OpenRouter API response received (status: {resp.status_code})")
            
            if resp.status_code != 200:
                logger.error(f"OpenRouter Error Body: {resp.text}")
                
            resp.raise_for_status()
            if stream:
                return resp
            data = resp.json()
            
            # å¤„ç† /responses å’Œ /chat/completions ä¸åŒçš„è¿”å›æ ¼å¼
            if reasoning:
                output = data.get("output")
                # å¦‚æœ output æ˜¯åˆ—è¡¨ï¼ˆOpenRouter /responses æ ‡å‡†éæµå¼æ ¼å¼ï¼‰
                if isinstance(output, list):
                    text_content = ""
                    for item in output:
                        if item.get("type") == "message":
                            content_list = item.get("content", [])
                            for c in content_list:
                                if c.get("type") == "output_text":
                                    text_content += c.get("text", "")
                    if text_content:
                        return text_content
                
                # å…œåº•é€»è¾‘ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™ç›´æ¥è¿”å›ï¼Œå¦åˆ™å°è¯• choices æ ¼å¼
                if isinstance(output, str):
                    return output
                return data.get("choices", [{}])[0].get("message", {}).get("content", "")
            else:
                return data["choices"][0]["message"]["content"]
        except RequestException as e:
            last_exception = e
            logger.warning(f"OpenRouter API attempt {attempt + 1} failed: {e}. Retrying...")
            if attempt < max_retries - 1:
                time.sleep(2 * (attempt + 1))
            
    raise RuntimeError(f"OpenRouter API request failed after {max_retries} attempts: {last_exception}")


def call_azure_openai(deployment: str, prompt: str, timeout: int = 60, max_retries: int = 3, stream: bool = False):
    """
    è°ƒç”¨ Azure OpenAI APIï¼ˆæ”¯æŒä¸­å›½åŒºå’Œå…¨çƒåŒºï¼‰
    
    Args:
        deployment: Azure OpenAI éƒ¨ç½²åç§°ï¼ˆä¸æ˜¯æ¨¡å‹åï¼‰
        prompt: è¾“å…¥æç¤ºè¯
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        stream: æ˜¯å¦æµå¼è¾“å‡º
    
    Azure ä¸­å›½åŒºå’Œå…¨çƒåŒºå·®å¼‚ï¼š
    - ä¸­å›½åŒº endpoint: https://{resource}.openai.azure.cn
    - å…¨çƒåŒº endpoint: https://{resource}.openai.azure.com
    - è®¤è¯æ–¹å¼ï¼šä½¿ç”¨ api-key headerï¼ˆä¸æ˜¯ Authorization: Bearerï¼‰
    - URL æ ¼å¼ï¼š/openai/deployments/{deployment}/chat/completions?api-version={version}
    """
    if not config.AZURE_OPENAI_API_KEY:
        raise ValueError("AZURE_OPENAI_API_KEY not configured")
    if not config.AZURE_OPENAI_ENDPOINT:
        raise ValueError("AZURE_OPENAI_ENDPOINT not configured")
    
    # æ„å»º Azure OpenAI URL
    url = (f"{config.AZURE_OPENAI_ENDPOINT}/openai/deployments/{deployment}"
           f"/chat/completions?api-version={config.AZURE_OPENAI_API_VERSION}")
    
    headers = {
        "api-key": config.AZURE_OPENAI_API_KEY,  # Azure ä½¿ç”¨ api-keyï¼Œä¸æ˜¯ Authorization
        "Content-Type": "application/json"
    }
    
    clean_prompt = "".join(c for c in prompt if c.isprintable() or c in "\n\r\t")
    logger.info(f"Azure OpenAI prompt length: {len(clean_prompt)} characters")
    
    payload = {
        "messages": [{"role": "user", "content": clean_prompt}],
        "stream": stream
    }
    
    last_exception = None
    for attempt in range(max_retries):
        try:
            logger.info(f"Azure OpenAI API call starting (attempt {attempt + 1}/{max_retries}, deployment={deployment}, stream={stream})...")
            resp = requests.post(url, json=payload, headers=headers, timeout=timeout, stream=stream)
            logger.info(f"Azure OpenAI API response received (status: {resp.status_code})")
            
            if resp.status_code != 200:
                logger.error(f"Azure OpenAI Error Body: {resp.text}")
                
            resp.raise_for_status()
            if stream:
                return resp
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except RequestException as e:
            last_exception = e
            logger.warning(f"Azure OpenAI API attempt {attempt + 1} failed: {e}. Retrying...")
            if attempt < max_retries - 1:
                time.sleep(2 * (attempt + 1))
    
    raise RuntimeError(f"Azure OpenAI API request failed after {max_retries} attempts: {last_exception}")


def call_anthropic(model: str, prompt: str, timeout: int = 60, max_retries: int = 3, stream: bool = False):
    """
    è°ƒç”¨ Anthropic Claude API
    
    Args:
        model: Claude æ¨¡å‹åç§°ï¼ˆå¦‚ claude-3-5-sonnet-20241022ï¼‰
        prompt: è¾“å…¥æç¤ºè¯
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        stream: æ˜¯å¦æµå¼è¾“å‡º
    
    Anthropic API ç‰¹ç‚¹ï¼š
    - Endpoint: https://api.anthropic.com/v1/messages
    - è®¤è¯æ–¹å¼ï¼šx-api-key header
    - APIç‰ˆæœ¬ï¼šé€šè¿‡ anthropic-version header æŒ‡å®š
    """
    if not config.ANTHROPIC_API_KEY:
        raise ValueError("ANTHROPIC_API_KEY not configured")
    
    url = f"{config.ANTHROPIC_BASE_URL}/v1/messages"
    
    headers = {
        "x-api-key": config.ANTHROPIC_API_KEY,
        "anthropic-version": getattr(config, 'ANTHROPIC_API_VERSION', '2023-06-01'),
        "Content-Type": "application/json"
    }
    
    clean_prompt = "".join(c for c in prompt if c.isprintable() or c in "\n\r\t")
    logger.info(f"Anthropic API prompt length: {len(clean_prompt)} characters")
    
    payload = {
        "model": model or config.ANTHROPIC_MODEL,
        "messages": [{"role": "user", "content": clean_prompt}],
        "max_tokens": 4096,
        "stream": stream
    }
    
    last_exception = None
    for attempt in range(max_retries):
        try:
            logger.info(f"Anthropic API call starting (attempt {attempt + 1}/{max_retries}, model={model}, stream={stream})...")
            resp = requests.post(url, json=payload, headers=headers, timeout=timeout, stream=stream)
            logger.info(f"Anthropic API response received (status: {resp.status_code})")
            
            if resp.status_code != 200:
                logger.error(f"Anthropic Error Body: {resp.text}")
                
            resp.raise_for_status()
            if stream:
                return resp
            data = resp.json()
            # Anthropic è¿”å›æ ¼å¼: {"content": [{"type": "text", "text": "..."}]}
            return data["content"][0]["text"]
        except RequestException as e:
            last_exception = e
            logger.warning(f"Anthropic API attempt {attempt + 1} failed: {e}. Retrying...")
            if attempt < max_retries - 1:
                time.sleep(2 * (attempt + 1))
    
    raise RuntimeError(f"Anthropic API request failed after {max_retries} attempts: {last_exception}")


def call_gemini(model: str, prompt: str, timeout: int = 60, max_retries: int = 3, stream: bool = False):
    """
    è°ƒç”¨ Google Gemini API
    
    Args:
        model: Gemini æ¨¡å‹åç§°ï¼ˆå¦‚ gemini-1.5-pro, gemini-1.5-flashï¼‰
        prompt: è¾“å…¥æç¤ºè¯
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        stream: æ˜¯å¦æµå¼è¾“å‡º
    
    Google Gemini API ç‰¹ç‚¹ï¼š
    - Endpoint: https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent
    - è®¤è¯æ–¹å¼ï¼šAPI key ä½œä¸º query å‚æ•°
    - æ”¯æŒæµå¼è¾“å‡ºï¼šstreamGenerateContent
    """
    if not config.GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY not configured")
    
    model_name = model or config.GEMINI_MODEL
    api_version = getattr(config, 'GEMINI_API_VERSION', 'v1beta')
    endpoint = 'streamGenerateContent' if stream else 'generateContent'
    url = f"{config.GEMINI_BASE_URL}/{api_version}/models/{model_name}:{endpoint}?key={config.GEMINI_API_KEY}"
    
    headers = {
        "Content-Type": "application/json"
    }
    
    clean_prompt = "".join(c for c in prompt if c.isprintable() or c in "\n\r\t")
    logger.info(f"Gemini API prompt length: {len(clean_prompt)} characters")
    
    payload = {
        "contents": [{
            "parts": [{"text": clean_prompt}]
        }],
        "generationConfig": {
            "temperature": 0.7,
            "maxOutputTokens": 8192
        }
    }
    
    last_exception = None
    for attempt in range(max_retries):
        try:
            logger.info(f"Gemini API call starting (attempt {attempt + 1}/{max_retries}, model={model_name}, stream={stream})...")
            resp = requests.post(url, json=payload, headers=headers, timeout=timeout, stream=stream)
            logger.info(f"Gemini API response received (status: {resp.status_code})")
            
            if resp.status_code != 200:
                logger.error(f"Gemini Error Body: {resp.text}")
                
            resp.raise_for_status()
            if stream:
                return resp
            data = resp.json()
            # Gemini è¿”å›æ ¼å¼: {"candidates": [{"content": {"parts": [{"text": "..."}]}}]}
            if data.get("candidates") and len(data["candidates"]) > 0:
                parts = data["candidates"][0].get("content", {}).get("parts", [])
                if parts and len(parts) > 0:
                    return parts[0].get("text", "")
            return ""
        except RequestException as e:
            last_exception = e
            logger.warning(f"Gemini API attempt {attempt + 1} failed: {e}. Retrying...")
            if attempt < max_retries - 1:
                time.sleep(2 * (attempt + 1))
    
    raise RuntimeError(f"Gemini API request failed after {max_retries} attempts: {last_exception}")


def get_openrouter_models():
    """è·å– OpenRouter æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ã€‚"""
    url = f"{config.OPENROUTER_BASE_URL.replace('/chat/completions', '')}/models"
    headers = {"Authorization": f"Bearer {config.OPENROUTER_API_KEY}"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        return resp.json().get("data", [])
    except Exception as e:
        logger.error(f"Failed to fetch OpenRouter models: {e}")
        return []


def get_deepseek_models():
    """è·å– DeepSeek æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ã€‚"""
    url = f"{config.DEEPSEEK_BASE_URL}/models"
    headers = {"Authorization": f"Bearer {config.DEEPSEEK_API_KEY}"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        return resp.json().get("data", [])
    except Exception as e:
        logger.error(f"Failed to fetch DeepSeek models: {e}")
        return []


def call_model(agent_id: str, prompt: str, model_config: dict = None, stream: bool = False):
    """ç»Ÿä¸€è°ƒç”¨æ¥å£ï¼š
    - model_config: {"type": "ollama"/"openai"/"deepseek", "model": "<model-name>"}
    - stream: æ˜¯å¦å¼€å¯æµå¼è¾“å‡º
    """
    if model_config is None:
        model_config = {"type": config.MODEL_BACKEND, "model": config.MODEL_NAME}
    
    mtype = model_config.get("type")
    mname = model_config.get("model")

    logger.info(f"Calling model backend: {mtype}, model: {mname}, stream: {stream}")

    if mtype == "ollama":
        if stream:
            return call_ollama_http(mname, prompt, stream=True)
        out = call_model_with_ollama_fallback(mname, prompt)
    elif mtype == "deepseek":
        res = call_deepseek(mname, prompt, stream=stream)
        if stream:
            return res
        out = res
    elif mtype == "aliyun":
        res = call_aliyun(mname, prompt, stream=stream)
        if stream:
            return res
        out = res
    elif mtype == "openai":
        res = call_openai(mname, prompt, stream=stream)
        if stream:
            return res
        out = res
    elif mtype == "openrouter":
        reasoning = model_config.get("reasoning")
        res = call_openrouter(mname, prompt, stream=stream, reasoning=reasoning)
        if stream:
            return res
        out = res
    elif mtype == "azure":
        # Azure OpenAI: ä½¿ç”¨éƒ¨ç½²åç§°ï¼ˆdeployment nameï¼‰ï¼Œä¸æ˜¯æ¨¡å‹å
        deployment = mname or config.AZURE_OPENAI_DEPLOYMENT_NAME
        res = call_azure_openai(deployment, prompt, stream=stream)
        if stream:
            return res
        out = res
    elif mtype == "anthropic":
        # Anthropic Claude API
        res = call_anthropic(mname, prompt, stream=stream)
        if stream:
            return res
        out = res
    elif mtype == "gemini":
        # Google Gemini API
        res = call_gemini(mname, prompt, stream=stream)
        if stream:
            return res
        out = res
    else:
        raise RuntimeError(f"Unsupported model backend: {mtype}")

    logger.info(f"Model response received (length: {len(out)})")
    logger.debug(f"Full raw response:\n{out}")

    # å°è¯•è§£æ JSON
    try:
        cleaned = out.strip()
        # å¯»æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
        start = cleaned.find('{')
        end = cleaned.rfind('}')
        if start != -1 and end != -1:
            json_candidate = cleaned[start:end+1]
            return json.loads(json_candidate)
        
        # å¦‚æœæ²¡æ‰¾åˆ° {}ï¼Œæˆ–è€…è§£æå¤±è´¥ï¼Œåˆ™è¿”å›åŸå§‹æ–‡æœ¬
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†å¼ºåˆ¶è¿”å› {"text": out}ï¼Œè€Œæ˜¯ç›´æ¥è¿”å›å­—ç¬¦ä¸²
        # è¿™æ ·é JSON ä»»åŠ¡ï¼ˆå¦‚ Reporterï¼‰èƒ½æ‹¿åˆ°åŸå§‹æ–‡æœ¬
        return out
    except Exception as e:
        logger.warning(f"JSON parsing failed: {e}. Returning raw output.")
        logger.error(traceback.format_exc())
        return out


# ========== Function Callingæ”¯æŒ ==========

def call_model_with_tools(agent_id: str, messages: list, model_config: dict = None, tools: list = None, max_tool_rounds: int = 5):
    """
    æ”¯æŒFunction Callingçš„æ¨¡å‹è°ƒç”¨ï¼ˆå¤šè½®å·¥å…·è°ƒç”¨å¾ªç¯ï¼‰
    
    Args:
        agent_id: Agentæ ‡è¯†ç¬¦
        messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant/tool", "content": "..."}]
        model_config: æ¨¡å‹é…ç½®
        tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆOpenAI Function Callingæ ¼å¼ï¼‰
        max_tool_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ¬¡
        
    Returns:
        æœ€ç»ˆçš„æ–‡æœ¬å“åº”æˆ–æ¶ˆæ¯å¯¹è±¡
    """
    if model_config is None:
        model_config = {"type": config.MODEL_BACKEND, "model": config.MODEL_NAME}
    
    mtype = model_config.get("type")
    mname = model_config.get("model")
    
    if not tools:
        # å¦‚æœæ²¡æœ‰å·¥å…·ï¼Œç›´æ¥è°ƒç”¨æ™®é€šæ¨¡å¼
        prompt = messages if isinstance(messages, list) else [{"role": "user", "content": messages}]
        return call_model(agent_id, prompt, model_config, stream=False)
    
    logger.info(f"[call_model_with_tools] Starting with {len(tools)} tools, max_tool_rounds={max_tool_rounds}")
    
    # å¯¼å…¥å·¥å…·æ‰§è¡Œå™¨
    from src.agents.meta_tools import execute_tool, format_tool_result_for_llm
    
    conversation = messages.copy()
    tool_round = 0
    
    while tool_round < max_tool_rounds:
        tool_round += 1
        logger.info(f"[call_model_with_tools] Round {tool_round}/{max_tool_rounds}")
        
        # è°ƒç”¨LLM
        if mtype == "deepseek":
            response = call_deepseek(mname, conversation, tools=tools, tool_choice="auto")
        elif mtype == "openai":
            response = call_openai(mname, conversation, tools=tools, tool_choice="auto")
        else:
            logger.warning(f"Model type {mtype} may not support function calling, falling back to normal mode")
            return call_model(agent_id, conversation, model_config, stream=False)
        
        # æ£€æŸ¥å“åº”ç±»å‹
        if isinstance(response, dict) and response.get("tool_calls"):
            # LLMè¦æ±‚è°ƒç”¨å·¥å…·
            tool_calls = response["tool_calls"]
            logger.info(f"[call_model_with_tools] LLM requested {len(tool_calls)} tool calls")
            
            # å°†assistantæ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²
            conversation.append({
                "role": "assistant",
                "content": response.get("content"),
                "tool_calls": tool_calls
            })
            
            # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            for tool_call in tool_calls:
                tool_name = tool_call["function"]["name"]
                tool_args_str = tool_call["function"]["arguments"]
                tool_id = tool_call["id"]
                
                try:
                    # è§£æå‚æ•°
                    tool_args = json.loads(tool_args_str) if isinstance(tool_args_str, str) else tool_args_str
                    logger.info(f"[call_model_with_tools] Executing tool: {tool_name} with args: {tool_args}")
                    
                    # å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶åˆ°Webç•Œé¢
                    try:
                        from src.agents.langchain_agents import send_web_event
                        import uuid
                        
                        # æ ¼å¼åŒ–å·¥å…·å‚æ•°æ˜¾ç¤º
                        args_preview = str(tool_args)[:200] + "..." if len(str(tool_args)) > 200 else str(tool_args)
                        
                        send_web_event(
                            "agent_action",
                            agent_name="è®®äº‹ç¼–æ’å®˜",
                            role_type="meta_orchestrator",
                            content=f"ğŸ”§ **è°ƒç”¨å·¥å…·**: `{tool_name}`\n\n**å‚æ•°**: {args_preview}",
                            chunk_id=str(uuid.uuid4())
                        )
                    except Exception as e:
                        logger.warning(f"Failed to send web event: {e}")
                    
                    # æ‰§è¡Œå·¥å…·
                    tool_result = execute_tool(tool_name, tool_args)
                    
                    # æ ¼å¼åŒ–ç»“æœ
                    result_text = format_tool_result_for_llm(tool_name, tool_result)
                    
                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
                    conversation.append({
                        "role": "tool",
                        "tool_call_id": tool_id,
                        "name": tool_name,
                        "content": result_text
                    })
                    
                    logger.info(f"[call_model_with_tools] Tool {tool_name} executed successfully")
                    
                    # å‘é€å·¥å…·æ‰§è¡ŒæˆåŠŸäº‹ä»¶
                    try:
                        from src.agents.langchain_agents import send_web_event
                        import uuid
                        
                        # æ ¼å¼åŒ–å·¥å…·ç»“æœé¢„è§ˆ
                        if isinstance(tool_result, dict):
                            if tool_result.get("success"):
                                status = "âœ… æˆåŠŸ"
                            else:
                                status = "âŒ å¤±è´¥"
                            result_preview = f"{status}: {str(tool_result)[:150]}..."
                        else:
                            result_preview = f"âœ… å®Œæˆ: {str(tool_result)[:150]}..."
                        
                        send_web_event(
                            "agent_action",
                            agent_name="è®®äº‹ç¼–æ’å®˜",
                            role_type="meta_orchestrator",
                            content=f"ğŸ”§ **å·¥å…·æ‰§è¡Œç»“æœ**: `{tool_name}`\n\n{result_preview}",
                            chunk_id=str(uuid.uuid4())
                        )
                    except Exception as e:
                        logger.warning(f"Failed to send web event: {e}")
                    
                except Exception as e:
                    logger.error(f"[call_model_with_tools] Tool {tool_name} execution failed: {str(e)}")
                    # æ·»åŠ é”™è¯¯ç»“æœ
                    conversation.append({
                        "role": "tool",
                        "tool_call_id": tool_id,
                        "name": tool_name,
                        "content": f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
                    })
            
            # ç»§ç»­å¾ªç¯ï¼Œè®©LLMå¤„ç†å·¥å…·ç»“æœ
            continue
        
        elif isinstance(response, dict) and response.get("content"):
            # LLMè¿”å›äº†æœ€ç»ˆæ–‡æœ¬å“åº”
            logger.info(f"[call_model_with_tools] LLM returned final response (length: {len(response['content'])})")
            return response["content"]
        
        elif isinstance(response, str):
            # ç›´æ¥è¿”å›çš„æ–‡æœ¬
            logger.info(f"[call_model_with_tools] LLM returned text response (length: {len(response)})")
            return response
        
        else:
            logger.warning(f"[call_model_with_tools] Unexpected response type: {type(response)}")
            return str(response)
    
    # è¾¾åˆ°æœ€å¤§è½®æ¬¡
    logger.warning(f"[call_model_with_tools] Reached max tool rounds ({max_tool_rounds}), returning last response")
    if isinstance(conversation[-1], dict) and conversation[-1].get("role") == "assistant":
        return conversation[-1].get("content", "")
    return "å·¥å…·è°ƒç”¨è¶…è¿‡æœ€å¤§è½®æ¬¡é™åˆ¶"


def parse_tool_calls_from_response(response: dict) -> list:
    """
    ä»LLMå“åº”ä¸­è§£æå·¥å…·è°ƒç”¨
    
    Args:
        response: LLM APIå“åº”å­—å…¸
        
    Returns:
        å·¥å…·è°ƒç”¨åˆ—è¡¨ [{"name": "tool_name", "arguments": {...}}]
    """
    if not isinstance(response, dict):
        return []
    
    tool_calls = response.get("tool_calls", [])
    if not tool_calls:
        return []
    
    parsed_calls = []
    for tc in tool_calls:
        try:
            func = tc.get("function", {})
            parsed_calls.append({
                "id": tc.get("id"),
                "name": func.get("name"),
                "arguments": json.loads(func.get("arguments", "{}")) if isinstance(func.get("arguments"), str) else func.get("arguments", {})
            })
        except Exception as e:
            logger.error(f"Failed to parse tool call: {tc}, error: {e}")
    
    return parsed_calls
